{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"datasets/CMaps/\")\n",
    "images_dir = \"images\"\n",
    "\n",
    "indexes = ['unit_number', 'time_cycles']\n",
    "settings = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensors = ['s_{}'.format(i+1) for i in range(0,21)]\n",
    "COLS = indexes + settings + sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensor_dictionary = {}\n",
    "dict_list = [\n",
    "    \"Fan intake temperature (°R)\",\n",
    "    \"Low-Pressure Compressor outlet temperature (°R)\",\n",
    "    \"High-Pressure Compressor outlet temperature (°R)\",\n",
    "    \"Low-Pressure Turbine outlet temperature (°R)\",\n",
    "    \"Fan intake pressure (psia)\",\n",
    "    \"Bypass-duct pressure (psia)\",\n",
    "    \"High-Pressure Compressor outlet pressure (psia)\",\n",
    "    \"Physical fan RPM\",\n",
    "    \"Physical core RPM\",\n",
    "    \"Engine pressure ratio (P50/P2)\",\n",
    "    \"High-Pressure Compressor outlet static pressure (psia)\",\n",
    "    \"Fuel flow to Ps30 ratio (pps/psia)\",\n",
    "    \"Corrected fan RPM\",\n",
    "    \"Corrected core RPM\",\n",
    "    \"Bypass ratio\",\n",
    "    \"Burner fuel-air ratio\",\n",
    "    \"Bleed enthalpy\",\n",
    "    \"Required fan RPM\",\n",
    "    \"Required fan conversion RPM\",\n",
    "    \"High-pressure turbine cooling airflow\",\n",
    "    \"Low-pressure turbine cooling airflow\"\n",
    "]\n",
    "\n",
    "Sensor_dictionary = {f's_{i+1}': sensor for i, sensor in enumerate(dict_list)}\n",
    "Sensor_dictionary\n",
    "\n",
    "def load_fd_dataset(dataset_id):\n",
    "\n",
    "    train_file = DATA_PATH / f\"train_FD00{dataset_id}.txt\"\n",
    "    test_file  = DATA_PATH / f\"test_FD00{dataset_id}.txt\"\n",
    "    rul_file   = DATA_PATH / f\"RUL_FD00{dataset_id}.txt\"\n",
    "\n",
    "    df_train = pd.read_csv(\n",
    "        train_file,\n",
    "        sep=r\"\\s+\",        \n",
    "        header=None,\n",
    "        names=COLS,\n",
    "        index_col=False\n",
    "    )\n",
    "\n",
    "    df_test = pd.read_csv(\n",
    "        test_file,\n",
    "        sep=r\"\\s+\",\n",
    "        header=None,\n",
    "        names=COLS,\n",
    "        index_col=False\n",
    "    )\n",
    "\n",
    "    df_rul = pd.read_csv(\n",
    "        rul_file,\n",
    "        sep=r\"\\s+\",\n",
    "        header=None,\n",
    "        names=[\"RUL\"],\n",
    "        index_col=False\n",
    "    )\n",
    "    \n",
    "    return df_train, df_test, df_rul\n",
    "\n",
    "def add_train_rul(df_train):\n",
    "    # Group by unit and get the max cycle of each engine\n",
    "    max_cycle = df_train.groupby(\"unit_number\")[\"time_cycles\"].transform(\"max\")\n",
    "    # RUL = distance to max cycle\n",
    "    df_train[\"RUL\"] = max_cycle - df_train[\"time_cycles\"]\n",
    "    return df_train\n",
    "\n",
    "def add_test_rul(df_test, df_rul):\n",
    "\n",
    "    idx = df_test.groupby(\"unit_number\")[\"time_cycles\"].transform(\"max\") == df_test[\"time_cycles\"]\n",
    "    final_test_rows = df_test[idx].copy().reset_index(drop=True)\n",
    "    final_test_rows[\"RUL\"] = df_rul[\"RUL\"]\n",
    "    \n",
    "    return final_test_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}  \n",
    "\n",
    "for i in range(1, 5):\n",
    "    \n",
    "    df_train_raw, df_test_raw, df_rul = load_fd_dataset(i)\n",
    "    df_train = add_train_rul(df_train_raw)\n",
    "    df_test_final = add_test_rul(df_test_raw, df_rul)\n",
    "    key = f\"FD00{i}\"\n",
    "    datasets[key] = {\n",
    "        \"train\":       df_train,   \n",
    "        \"test\":        df_test_raw,\n",
    "        \"rul\":         df_rul,\n",
    "        \"test_final\":  df_test_final,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\n\\n# Prepare features and target\\ndf_fd001 = datasets[\"FD001\"][\"train\"]\\nX = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\\ny = df_fd001[\"RUL\"]\\n\\n# Split into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Initialize and train Gradient Boosting Regressor\\ngbr = GradientBoostingRegressor(\\n    n_estimators=200,\\n    learning_rate=0.1,\\n    max_depth=5,\\n    random_state=42\\n)\\ngbr.fit(X_train, y_train)\\n\\n# Make predictions\\ny_pred = gbr.predict(X_val)\\n\\n# Evaluate\\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\\nmae = mean_absolute_error(y_val, y_pred)\\nr2 = r2_score(y_val, y_pred)\\n\\nprint(\"RMSE:\", rmse)\\nprint(\"MAE:\", mae)\\nprint(\"R²:\", r2) '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Prepare features and target\n",
    "df_fd001 = datasets[\"FD001\"][\"train\"]\n",
    "X = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\n",
    "y = df_fd001[\"RUL\"]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbr.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R²:\", r2) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\nimport numpy as np\\n\\n# Prepare features and target\\ndf_fd001 = datasets[\"FD001\"][\"train\"]\\n\\n# Drop unnecessary columns\\ndrop_cols = [\"unit_number\", \"time_cycles\", \"RUL\", \"s_1\", \"s_5\", \"s_6\", \"s_10\", \"s_16\", \"s_18\", \"s_19\"]\\nfeature_cols = [col for col in df_fd001.columns if col not in drop_cols]\\nX = df_fd001[feature_cols]\\ny = df_fd001[\"RUL\"]\\n\\n# Split into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Initialize and train Gradient Boosting Regressor\\ngbr = GradientBoostingRegressor(\\n    n_estimators=200,\\n    learning_rate=0.1,\\n    max_depth=5,\\n    random_state=42\\n)\\ngbr.fit(X_train, y_train)\\n\\n# Make predictions\\ny_pred = gbr.predict(X_val)\\n\\n# Evaluate\\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\\nmae = mean_absolute_error(y_val, y_pred)\\nr2 = r2_score(y_val, y_pred)\\n\\nprint(\"RMSE:\", rmse)\\nprint(\"MAE:\", mae)\\nprint(\"R²:\", r2) '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Prepare features and target\n",
    "df_fd001 = datasets[\"FD001\"][\"train\"]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "drop_cols = [\"unit_number\", \"time_cycles\", \"RUL\", \"s_1\", \"s_5\", \"s_6\", \"s_10\", \"s_16\", \"s_18\", \"s_19\"]\n",
    "feature_cols = [col for col in df_fd001.columns if col not in drop_cols]\n",
    "X = df_fd001[feature_cols]\n",
    "y = df_fd001[\"RUL\"]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbr.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R²:\", r2) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import numpy as np\\nimport pandas as pd\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\nfrom scipy.stats import uniform, randint\\n\\n# Prepare features and target\\ndf_fd001 = datasets[\"FD001\"][\"train\"]\\nX = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\\ny = df_fd001[\"RUL\"]\\n\\n# Split into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Initialize the Gradient Boosting Regressor\\ngbr = GradientBoostingRegressor(random_state=42)\\n\\n# Set up the hyperparameter search space\\nparam_dist = {\\n    \\'n_estimators\\': randint(100, 1000),  # number of trees\\n    \\'learning_rate\\': uniform(0.01, 0.2),  # learning rate\\n    \\'max_depth\\': randint(3, 10),  # max depth of the trees\\n    \\'min_samples_split\\': randint(2, 10),  # min samples to split\\n    \\'min_samples_leaf\\': randint(1, 10),  # min samples to be at a leaf node\\n    \\'subsample\\': uniform(0.5, 0.5)  # fraction of samples to use for fitting each tree\\n}\\n\\n# Set up RandomizedSearchCV\\nrandom_search = RandomizedSearchCV(\\n    gbr, param_distributions=param_dist, \\n    n_iter=50,  # number of different combinations to try\\n    scoring=\\'neg_mean_squared_error\\',  # use negative MSE for scoring (as lower is better)\\n    cv=5,  # 5-fold cross-validation\\n    verbose=1,  # print progress\\n    random_state=42,\\n    n_jobs=-1  # use all available cores for computation\\n)\\n\\n# Run the RandomizedSearchCV\\nrandom_search.fit(X_train, y_train)\\n\\n# Get the best model from the random search\\nbest_gbr = random_search.best_estimator_\\n\\n# Make predictions with the best model\\ny_pred = best_gbr.predict(X_val)\\n\\n# Evaluate the model\\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\\nmae = mean_absolute_error(y_val, y_pred)\\nr2 = r2_score(y_val, y_pred)\\n\\nprint(\"Best Hyperparameters:\", random_search.best_params_)\\nprint(\"RMSE:\", rmse)\\nprint(\"MAE:\", mae)\\nprint(\"R²:\", r2)\\n '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Prepare features and target\n",
    "df_fd001 = datasets[\"FD001\"][\"train\"]\n",
    "X = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\n",
    "y = df_fd001[\"RUL\"]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Set up the hyperparameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),  # number of trees\n",
    "    'learning_rate': uniform(0.01, 0.2),  # learning rate\n",
    "    'max_depth': randint(3, 10),  # max depth of the trees\n",
    "    'min_samples_split': randint(2, 10),  # min samples to split\n",
    "    'min_samples_leaf': randint(1, 10),  # min samples to be at a leaf node\n",
    "    'subsample': uniform(0.5, 0.5)  # fraction of samples to use for fitting each tree\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    gbr, param_distributions=param_dist, \n",
    "    n_iter=50,  # number of different combinations to try\n",
    "    scoring='neg_mean_squared_error',  # use negative MSE for scoring (as lower is better)\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1,  # print progress\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # use all available cores for computation\n",
    ")\n",
    "\n",
    "# Run the RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the random search\n",
    "best_gbr = random_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_gbr.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R²:\", r2)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " unit_number    0\n",
      "time_cycles    0\n",
      "setting_1      0\n",
      "setting_2      0\n",
      "setting_3      0\n",
      "s_1            0\n",
      "s_2            0\n",
      "s_3            0\n",
      "s_4            0\n",
      "s_5            0\n",
      "s_6            0\n",
      "s_7            0\n",
      "s_8            0\n",
      "s_9            0\n",
      "s_10           0\n",
      "s_11           0\n",
      "s_12           0\n",
      "s_13           0\n",
      "s_14           0\n",
      "s_15           0\n",
      "s_16           0\n",
      "s_17           0\n",
      "s_18           0\n",
      "s_19           0\n",
      "s_20           0\n",
      "s_21           0\n",
      "RUL            0\n",
      "dtype: int64\n",
      "RMSE: 41.772032455442535\n",
      "MAE: 31.736391922615862\n",
      "R²: 0.5311458990633546\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "df_fd001 = datasets[\"FD001\"][\"train\"]\n",
    "\n",
    "# 1. Handle missing values\n",
    "# Checking for missing values in the dataset\n",
    "missing_data = df_fd001.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_data)\n",
    "\n",
    "# If there are missing values, you can either drop them or fill them. \n",
    "# Let's fill missing values with the median (this can be changed depending on the context).\n",
    "df_fd001 = df_fd001.fillna(df_fd001.median())\n",
    "\n",
    "# 2. Outlier detection and removal\n",
    "# Outliers can be detected using z-scores, IQR, or domain knowledge. \n",
    "# Here, we'll use IQR to remove extreme outliers for each feature.\n",
    "\n",
    "# Calculate the Interquartile Range (IQR)\n",
    "Q1 = df_fd001.quantile(0.25)\n",
    "Q3 = df_fd001.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Remove outliers (values outside 1.5*IQR from the 25th and 75th percentiles)\n",
    "df_fd001 = df_fd001[~((df_fd001 < (Q1 - 1.5 * IQR)) | (df_fd001 > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# 3. Feature scaling\n",
    "# If you're using models sensitive to scale, such as linear regression or k-NN, scaling is important.\n",
    "# StandardScaler works well for most models. We will apply it here.\n",
    "\n",
    "# Dropping non-numeric columns that shouldn't be scaled (like unit_number and time_cycles)\n",
    "X = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Target variable\n",
    "y = df_fd001[\"RUL\"]\n",
    "\n",
    "# 4. Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now you can proceed with training your model, e.g., Gradient Boosting or other models\n",
    "\n",
    "# Example: Gradient Boosting Regressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize and train Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbr.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R²:\", r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
