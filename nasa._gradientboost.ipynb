{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"datasets/CMaps/\")\n",
    "images_dir = \"images\"\n",
    "\n",
    "indexes = ['unit_number', 'time_cycles']\n",
    "settings = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensors = ['s_{}'.format(i+1) for i in range(0,21)]\n",
    "COLS = indexes + settings + sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensor_dictionary = {}\n",
    "dict_list = [\n",
    "    \"Fan intake temperature (°R)\",\n",
    "    \"Low-Pressure Compressor outlet temperature (°R)\",\n",
    "    \"High-Pressure Compressor outlet temperature (°R)\",\n",
    "    \"Low-Pressure Turbine outlet temperature (°R)\",\n",
    "    \"Fan intake pressure (psia)\",\n",
    "    \"Bypass-duct pressure (psia)\",\n",
    "    \"High-Pressure Compressor outlet pressure (psia)\",\n",
    "    \"Physical fan RPM\",\n",
    "    \"Physical core RPM\",\n",
    "    \"Engine pressure ratio (P50/P2)\",\n",
    "    \"High-Pressure Compressor outlet static pressure (psia)\",\n",
    "    \"Fuel flow to Ps30 ratio (pps/psia)\",\n",
    "    \"Corrected fan RPM\",\n",
    "    \"Corrected core RPM\",\n",
    "    \"Bypass ratio\",\n",
    "    \"Burner fuel-air ratio\",\n",
    "    \"Bleed enthalpy\",\n",
    "    \"Required fan RPM\",\n",
    "    \"Required fan conversion RPM\",\n",
    "    \"High-pressure turbine cooling airflow\",\n",
    "    \"Low-pressure turbine cooling airflow\"\n",
    "]\n",
    "\n",
    "Sensor_dictionary = {f's_{i+1}': sensor for i, sensor in enumerate(dict_list)}\n",
    "Sensor_dictionary\n",
    "\n",
    "def load_fd_dataset(dataset_id):\n",
    "\n",
    "    train_file = DATA_PATH / f\"train_FD00{dataset_id}.txt\"\n",
    "    test_file  = DATA_PATH / f\"test_FD00{dataset_id}.txt\"\n",
    "    rul_file   = DATA_PATH / f\"RUL_FD00{dataset_id}.txt\"\n",
    "\n",
    "    df_train = pd.read_csv(\n",
    "        train_file,\n",
    "        sep=r\"\\s+\",        \n",
    "        header=None,\n",
    "        names=COLS,\n",
    "        index_col=False\n",
    "    )\n",
    "\n",
    "    df_test = pd.read_csv(\n",
    "        test_file,\n",
    "        sep=r\"\\s+\",\n",
    "        header=None,\n",
    "        names=COLS,\n",
    "        index_col=False\n",
    "    )\n",
    "\n",
    "    df_rul = pd.read_csv(\n",
    "        rul_file,\n",
    "        sep=r\"\\s+\",\n",
    "        header=None,\n",
    "        names=[\"RUL\"],\n",
    "        index_col=False\n",
    "    )\n",
    "    \n",
    "    return df_train, df_test, df_rul\n",
    "\n",
    "def add_train_rul(df_train):\n",
    "    # Group by unit and get the max cycle of each engine\n",
    "    max_cycle = df_train.groupby(\"unit_number\")[\"time_cycles\"].transform(\"max\")\n",
    "    # RUL = distance to max cycle\n",
    "    df_train[\"RUL\"] = max_cycle - df_train[\"time_cycles\"]\n",
    "    return df_train\n",
    "\n",
    "def add_test_rul(df_test, df_rul):\n",
    "\n",
    "    idx = df_test.groupby(\"unit_number\")[\"time_cycles\"].transform(\"max\") == df_test[\"time_cycles\"]\n",
    "    final_test_rows = df_test[idx].copy().reset_index(drop=True)\n",
    "    final_test_rows[\"RUL\"] = df_rul[\"RUL\"]\n",
    "    \n",
    "    return final_test_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}  \n",
    "\n",
    "for i in range(1, 5):\n",
    "    \n",
    "    df_train_raw, df_test_raw, df_rul = load_fd_dataset(i)\n",
    "    df_train = add_train_rul(df_train_raw)\n",
    "    df_test_final = add_test_rul(df_test_raw, df_rul)\n",
    "    key = f\"FD00{i}\"\n",
    "    datasets[key] = {\n",
    "        \"train\":       df_train,   \n",
    "        \"test\":        df_test_raw,\n",
    "        \"rul\":         df_rul,\n",
    "        \"test_final\":  df_test_final,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\n\\n# Prepare features and target\\ndf_fd001 = datasets[\"FD001\"][\"train\"]\\nX = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\\ny = df_fd001[\"RUL\"]\\n\\n# Split into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Initialize and train Gradient Boosting Regressor\\ngbr = GradientBoostingRegressor(\\n    n_estimators=200,\\n    learning_rate=0.1,\\n    max_depth=5,\\n    random_state=42\\n)\\ngbr.fit(X_train, y_train)\\n\\n# Make predictions\\ny_pred = gbr.predict(X_val)\\n\\n# Evaluate\\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\\nmae = mean_absolute_error(y_val, y_pred)\\nr2 = r2_score(y_val, y_pred)\\n\\nprint(\"RMSE:\", rmse)\\nprint(\"MAE:\", mae)\\nprint(\"R²:\", r2)\\n '"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Prepare features and target\n",
    "df_fd001 = datasets[\"FD001\"][\"train\"]\n",
    "X = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\n",
    "y = df_fd001[\"RUL\"]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbr.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R²:\", r2)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest gradient boost evulation with no added features or methods to see what it looks on basic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\nimport numpy as np\\n\\n# Prepare features and target\\ndf_fd001 = datasets[\"FD001\"][\"train\"]\\n\\n# Drop unnecessary columns\\ndrop_cols = [\"unit_number\", \"time_cycles\", \"RUL\", \"s_1\", \"s_5\", \"s_6\", \"s_10\", \"s_16\", \"s_18\", \"s_19\"]\\nfeature_cols = [col for col in df_fd001.columns if col not in drop_cols]\\nX = df_fd001[feature_cols]\\ny = df_fd001[\"RUL\"]\\n\\n# Split into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Initialize and train Gradient Boosting Regressor\\ngbr = GradientBoostingRegressor(\\n    n_estimators=200,\\n    learning_rate=0.1,\\n    max_depth=5,\\n    random_state=42\\n)\\ngbr.fit(X_train, y_train)\\n\\n# Make predictions\\ny_pred = gbr.predict(X_val)\\n\\n# Evaluate\\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\\nmae = mean_absolute_error(y_val, y_pred)\\nr2 = r2_score(y_val, y_pred)\\n\\nprint(\"RMSE:\", rmse)\\nprint(\"MAE:\", mae)\\nprint(\"R²:\", r2) '"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Prepare features and target\n",
    "df_fd001 = datasets[\"FD001\"][\"train\"]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "drop_cols = [\"unit_number\", \"time_cycles\", \"RUL\", \"s_1\", \"s_5\", \"s_6\", \"s_10\", \"s_16\", \"s_18\", \"s_19\"]\n",
    "feature_cols = [col for col in df_fd001.columns if col not in drop_cols]\n",
    "X = df_fd001[feature_cols]\n",
    "y = df_fd001[\"RUL\"]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbr.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R²:\", r2) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we do dropping some slightlt unhelpful columns our values worsen somehow so it not may be best to not drop those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\nfrom sklearn.preprocessing import StandardScaler\\nimport numpy as np\\n\\n# Prepare features and target\\ndf_fd001 = datasets[\"FD001\"][\"train\"]\\nX = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\\ny = df_fd001[\"RUL\"]\\n\\n# Scale features\\nscaler = StandardScaler()\\nX_scaled = scaler.fit_transform(X)\\n\\n# Split into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\\n\\n# Initialize and train Gradient Boosting Regressor\\ngbr = GradientBoostingRegressor(\\n    n_estimators=200,\\n    learning_rate=0.1,\\n    max_depth=5,\\n    random_state=42\\n)\\ngbr.fit(X_train, y_train)\\n\\n# Make predictions\\ny_pred = gbr.predict(X_val)\\n\\n# Evaluate\\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\\nmae = mean_absolute_error(y_val, y_pred)\\nr2 = r2_score(y_val, y_pred)\\n\\nprint(\"RMSE:\", rmse)\\nprint(\"MAE:\", mae)\\nprint(\"R²:\", r2)\\n '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Prepare features and target\n",
    "df_fd001 = datasets[\"FD001\"][\"train\"]\n",
    "X = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\n",
    "y = df_fd001[\"RUL\"]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbr.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R²:\", r2)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see scaling helps with results so it will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import numpy as np\\nimport pandas as pd\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\nfrom scipy.stats import uniform, randint\\n\\n# Prepare features and target\\ndf_fd001 = datasets[\"FD001\"][\"train\"]\\nX = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\\ny = df_fd001[\"RUL\"]\\n\\n# Split into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Initialize the Gradient Boosting Regressor\\ngbr = GradientBoostingRegressor(random_state=42)\\n\\n# Set up the hyperparameter search space\\nparam_dist = {\\n    \\'n_estimators\\': randint(100, 1000),  # number of trees\\n    \\'learning_rate\\': uniform(0.01, 0.2),  # learning rate\\n    \\'max_depth\\': randint(3, 10),  # max depth of the trees\\n    \\'min_samples_split\\': randint(2, 10),  # min samples to split\\n    \\'min_samples_leaf\\': randint(1, 10),  # min samples to be at a leaf node\\n    \\'subsample\\': uniform(0.5, 0.5)  # fraction of samples to use for fitting each tree\\n}\\n\\n# Set up RandomizedSearchCV\\nrandom_search = RandomizedSearchCV(\\n    gbr, param_distributions=param_dist, \\n    n_iter=50,  # number of different combinations to try\\n    scoring=\\'neg_mean_squared_error\\',  # use negative MSE for scoring (as lower is better)\\n    cv=5,  # 5-fold cross-validation\\n    verbose=1,  # print progress\\n    random_state=42,\\n    n_jobs=-1  # use all available cores for computation\\n)\\n\\n# Run the RandomizedSearchCV\\nrandom_search.fit(X_train, y_train)\\n\\n# Get the best model from the random search\\nbest_gbr = random_search.best_estimator_\\n\\n# Make predictions with the best model\\ny_pred = best_gbr.predict(X_val)\\n\\n# Evaluate the model\\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\\nmae = mean_absolute_error(y_val, y_pred)\\nr2 = r2_score(y_val, y_pred)\\n\\nprint(\"Best Hyperparameters:\", random_search.best_params_)\\nprint(\"RMSE:\", rmse)\\nprint(\"MAE:\", mae)\\nprint(\"R²:\", r2)\\n '"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Prepare features and target\n",
    "df_fd001 = datasets[\"FD001\"][\"train\"]\n",
    "X = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\n",
    "y = df_fd001[\"RUL\"]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Set up the hyperparameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),  # number of trees\n",
    "    'learning_rate': uniform(0.01, 0.2),  # learning rate\n",
    "    'max_depth': randint(3, 10),  # max depth of the trees\n",
    "    'min_samples_split': randint(2, 10),  # min samples to split\n",
    "    'min_samples_leaf': randint(1, 10),  # min samples to be at a leaf node\n",
    "    'subsample': uniform(0.5, 0.5)  # fraction of samples to use for fitting each tree\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    gbr, param_distributions=param_dist, \n",
    "    n_iter=50,  # number of different combinations to try\n",
    "    scoring='neg_mean_squared_error',  # use negative MSE for scoring (as lower is better)\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1,  # print progress\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # use all available cores for computation\n",
    ")\n",
    "\n",
    "# Run the RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the random search\n",
    "best_gbr = random_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_gbr.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R²:\", r2)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used grid search to see best hyperparameters to see what values are best. Which with the best model resluts will be used on next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 41.01073497313826\n",
      "MAE: 29.34156323192828\n",
      "R²: 0.6318766026617582\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Load dataset\n",
    "df_fd001 = datasets[\"FD001\"][\"train\"]\n",
    "\n",
    "# Features and target\n",
    "X = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\n",
    "y = df_fd001[\"RUL\"]\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model with best found hyperparameters\n",
    "gbr = GradientBoostingRegressor(\n",
    "    learning_rate=0.010155753168202867,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=559,\n",
    "    subsample=0.8058265802441404,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = gbr.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R²:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using best hyperparameters we found on GridSearch and use Scaling combined we get best results yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the final engine cycle:\n",
      "RMSE: 5.289838019490726\n",
      "MAE: 5.052275305823042\n",
      "R²: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Load dataset\n",
    "df_fd001 = datasets[\"FD001\"][\"train\"]\n",
    "\n",
    "# Filter only the last cycle for each engine\n",
    "df_last_cycle = df_fd001.loc[df_fd001.groupby('unit_number')['time_cycles'].idxmax()]\n",
    "\n",
    "# Features and target (using the whole dataset for training, but predicting on the last cycle only)\n",
    "X = df_fd001.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"])\n",
    "y = df_fd001[\"RUL\"]\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data (training using the whole data and testing on the last cycle only)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model with best found hyperparameters\n",
    "gbr = GradientBoostingRegressor(\n",
    "    learning_rate=0.010155753168202867,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=559,\n",
    "    subsample=0.8058265802441404,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the last cycle only\n",
    "X_last_cycle = scaler.transform(df_last_cycle.drop(columns=[\"unit_number\", \"time_cycles\", \"RUL\"]))\n",
    "y_last_cycle_true = df_last_cycle[\"RUL\"]\n",
    "\n",
    "# Predict\n",
    "y_last_cycle_pred = gbr.predict(X_last_cycle)\n",
    "\n",
    "# Evaluate on the last cycle only\n",
    "rmse_last_cycle = np.sqrt(mean_squared_error(y_last_cycle_true, y_last_cycle_pred))\n",
    "mae_last_cycle = mean_absolute_error(y_last_cycle_true, y_last_cycle_pred)\n",
    "r2_last_cycle = r2_score(y_last_cycle_true, y_last_cycle_pred)\n",
    "\n",
    "print(\"Evaluation on the final engine cycle:\")\n",
    "print(\"RMSE:\", rmse_last_cycle)\n",
    "print(\"MAE:\", mae_last_cycle)\n",
    "print(\"R²:\", r2_last_cycle)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
